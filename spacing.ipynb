{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import module required|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Flatten, Embedding, Concatenate, Conv1D, BatchNormalization, TimeDistributed, GRU, Reshape\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from model import CNNBasedRNN\n",
    "from model import RNN\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preprocessing and build word2idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH=200\n",
    "def hangulExtractor(str):\n",
    "    hangul = re.compile('[^ !?.$ㄱ-ㅎㅣ가-힣|a-z|A-Z]+') # 한글과 영어 띄어쓰기 중요 문장부호를 제외한 모든 글자\n",
    "    # hangul = re.compile('[^ \\u3131-\\u3163\\uac00-\\ud7a3]+')  \n",
    "    result = hangul.sub('', str)\n",
    "    return result\n",
    "rawfile = 'raw_spacing_corpus.txt'\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def generate_word2idx():\n",
    "    tmp_corpus=[]\n",
    "    word2idx={}\n",
    "    with open(rawfile,'r') as fp:\n",
    "        for line in fp:\n",
    "            line=hangulExtractor(line)\n",
    "            tmp_corpus.extend(list(line.rstrip().replace(' ','')))\n",
    "        \n",
    "    for idx, (char, freq) in enumerate(Counter(tmp_corpus).most_common(),1):\n",
    "        word2idx[char]=idx\n",
    "    \n",
    "    def save_obj(obj, word2idx_file):\n",
    "        with open(word2idx_file,'wb') as f:\n",
    "            pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "    save_obj(word2idx,'word2idx.pkl')\n",
    "    \n",
    "    return word2idx\n",
    "\n",
    "word2idx = generate_word2idx()\n",
    "EMBEDDING_DIM = 100\n",
    "DIC_SIZE = len(word2idx)\n",
    "embedding_matrix = np.random.random((len(word2idx)+1, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data loading and and convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_corpus():\n",
    "    Filename = 'raw_spacing_corpus.txt'\n",
    "    raw_data=[]\n",
    "    with open(Filename,'r') as f:\n",
    "        for line in f:\n",
    "            raw_data.append(hangulExtractor(line.rstrip()))\n",
    "    return raw_data\n",
    "\n",
    "def generate_trainset(raw_data):\n",
    "    \"\"\" prepare data set for training \n",
    "    X: I have an apple  Y: 122212122221\n",
    "    Padding X and Y\n",
    "    X: 00000000Ihaveanapple and Y:00000000122212122221\n",
    "    \"\"\"\n",
    "    \n",
    "    X,Y=[],[]\n",
    "    for sent in raw_data:\n",
    "        sent=sent.replace(' ','^')# '^' refer to the space\n",
    "        segmented = list(sent)\n",
    "        \n",
    "        if len(segmented)<=MAX_SEQUENCE_LENGTH:\n",
    "            tmp_x, tmp_y = [],[]\n",
    "            tmp_length=len(segmented)\n",
    "            for idx in range(tmp_length):\n",
    "            \n",
    "                if idx < tmp_length-1:\n",
    "                    if segmented[idx] != '^':\n",
    "                        tmp_x.append(word2idx[segmented[idx]])\n",
    "                    \n",
    "                        if idx+1 < tmp_length:\n",
    "                            if segmented[idx+1] == '^':\n",
    "                                tmp_y.append(1) \n",
    "                            else:\n",
    "                                tmp_y.append(0)\n",
    "                        else:\n",
    "                            tmp_y.append(0)\n",
    "                elif idx == tmp_length-1:\n",
    "                    tmp_x.append(word2idx[segmented[idx]])\n",
    "                    tmp_y.append(1)\n",
    "            Y.append(tmp_y)\n",
    "            X.append(tmp_x)\n",
    "            tmp_x, tmp_x = [],[]        \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_corpus()\n",
    "X,Y = generate_trainset(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH )\n",
    "Y_train = pad_sequences(Y, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "Y_train = to_categorical(Y_train, num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Construction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sghan/deeplearning_model/spacing/model.py:40: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ti...)`\n",
      "  model = Model(inputs=seq_input, output=b)\n"
     ]
    }
   ],
   "source": [
    "model = RNN(MAX_SEQUENCE_LENGTH=MAX_SEQUENCE_LENGTH, DIC_SIZE=DIC_SIZE, EMBEDDING_DIM=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100540/100540 [==============================] - 51s 510us/step - loss: 0.5858 - acc: 0.7482\n",
      "Epoch 2/10\n",
      "100540/100540 [==============================] - 50s 499us/step - loss: 0.3198 - acc: 0.8669\n",
      "Epoch 3/10\n",
      "100540/100540 [==============================] - 49s 492us/step - loss: 0.2736 - acc: 0.8862\n",
      "Epoch 4/10\n",
      "100540/100540 [==============================] - 50s 500us/step - loss: 0.2538 - acc: 0.8947\n",
      "Epoch 5/10\n",
      "100540/100540 [==============================] - 50s 497us/step - loss: 0.2452 - acc: 0.8983\n",
      "Epoch 6/10\n",
      "100540/100540 [==============================] - 50s 495us/step - loss: 0.2369 - acc: 0.9021\n",
      "Epoch 7/10\n",
      "100540/100540 [==============================] - 50s 498us/step - loss: 0.2286 - acc: 0.9058\n",
      "Epoch 8/10\n",
      "100540/100540 [==============================] - 50s 499us/step - loss: 0.2207 - acc: 0.9093\n",
      "Epoch 9/10\n",
      "100540/100540 [==============================] - 50s 499us/step - loss: 0.2128 - acc: 0.9127\n",
      "Epoch 10/10\n",
      "100540/100540 [==============================] - 49s 490us/step - loss: 0.2051 - acc: 0.9160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdcbb8ff5f8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size=1024, epochs=10)\n",
    "##TODO make padded dataset for both  X,Y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('spacing_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합계 17M\r\n",
      "drwxrwxr-x 4 sghan sghan 4.0K 10월 26 01:07 .\r\n",
      "drwxrwxr-x 6 sghan sghan 4.0K 10월 24 18:27 ..\r\n",
      "drwxrwxr-x 2 sghan sghan 4.0K 10월 25 22:54 .ipynb_checkpoints\r\n",
      "-rw-r--r-- 1 sghan sghan  12K 10월 26 00:58 .model.py.swp\r\n",
      "-rw-rw-r-- 1 sghan sghan 2.0K 10월 24 18:43 Untitled.ipynb\r\n",
      "-rw-rw-r-- 1 sghan sghan 1.7K 10월 26 00:58 Untitled1.ipynb\r\n",
      "drwxrwxr-x 2 sghan sghan 4.0K 10월 26 00:58 __pycache__\r\n",
      "-rw-rw-r-- 1 sghan sghan 2.0K 10월 26 00:58 model.py\r\n",
      "-rw-rw-r-- 1 sghan sghan  30K 10월 26 00:58 model_plot.png\r\n",
      "-rw-rw-r-- 1 sghan sghan  13M 10월 24 18:27 raw_spacing_corpus.txt\r\n",
      "-rw-rw-r-- 1 sghan sghan 2.8K 10월 24 18:27 sample.txt\r\n",
      "-rw-rw-r-- 1 sghan sghan 8.1K 10월 26 01:06 spacing.ipynb\r\n",
      "-rw-rw-r-- 1 sghan sghan 4.2M 10월 26 01:07 spacing_model.h5\r\n",
      "-rw-rw-r-- 1 sghan sghan  17K 10월 24 18:27 word2idx\r\n",
      "-rw-rw-r-- 1 sghan sghan  17K 10월 26 00:58 word2idx.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
